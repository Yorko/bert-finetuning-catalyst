# BERT fine-tuning with PyTorch, HuggingFace, and Catalyst

<img src='https://habrastorage.org/webt/ne/n_/ow/nen_ow49hxu8zrkgolq1rv3xkhi.png'>

**Instruction:**

- specify your data, model, and training parameters in `config.yml`
- if needed, customize the code for data processing in `src/data.py`
- specify your model in `src/model.py`, by default it's DistilBERT for sequence classification
- run `python src/train.py`

Also, see more extended tutorials:

 - multi-class classification: classifying Amazon product reviews into categories, [Kaggle Notebook](https://www.kaggle.com/kashnitsky/distillbert-catalyst-amazon-product-reviews)
 - multi-label classification: identifying toxic comments, [Kaggle Notebook](https://www.kaggle.com/kashnitsky/catalyst-distilbert-multilabel-clf-toxic-comments)

 
[Confluence page](https://confluence.cbsels.com/display/RCODS/BERT+fine-tuning+with+PyTorch%2C+HuggingFace%2C+and+Catalyst), [recording](https://elsevier.zoom.us/rec/share/17g_YJO4Zfz_PoO9h1r_PNmsefJGnrlE6GCEsXmmeJBJ3gy6yjXTpNZWyIG4t8dy.UPdxNzYRzSArbJi7) (passcode: 1sfBi?xP)
